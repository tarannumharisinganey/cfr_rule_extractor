{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba14d2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- STEP 1: CLEAN ENVIRONMENT & INSTALL ---\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import re\n",
    "\n",
    "print(\"üßπ Cleaning environment (removing conflicting libraries)...\")\n",
    "# Uninstall conflicts quietly\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"jax\", \"jaxlib\", \"tensorflow\", \"chex\", \"flax\"], capture_output=True)\n",
    "\n",
    "print(\"üì¶ Installing dependencies...\")\n",
    "# Install marker and boto3\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"marker-pdf\", \"boto3\"], capture_output=True)\n",
    "print(\"‚úì Environment ready.\\n\")\n",
    "\n",
    "\n",
    "# --- STEP 2: MAIN PROCESSING SCRIPT ---\n",
    "import boto3\n",
    "import shutil\n",
    "import time\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "# Force CPU settings globally\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ[\"TORCH_DEVICE\"] = \"cpu\"\n",
    "\n",
    "# AWS Configuration\n",
    "BUCKET_NAME = 'tarannumpdf'\n",
    "INPUT_FOLDER = 'finra/'\n",
    "OUTPUT_FOLDER = 'output/'\n",
    "\n",
    "# --- CREDENTIALS ---\n",
    "aws_access_key = \"add your own\"\n",
    "aws_secret_key = \"ADD YOUR OWN\"\n",
    "aws_region = \"ap-south-1\"\n",
    "\n",
    "# Initialize S3 client\n",
    "print(f\"Connecting to AWS S3 ({aws_region})...\")\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    region_name=aws_region\n",
    ")\n",
    "print(f\"‚úì Connected to AWS S3\")\n",
    "\n",
    "# Create local directories\n",
    "os.makedirs('pdfs', exist_ok=True)\n",
    "os.makedirs('markdown_output', exist_ok=True)\n",
    "\n",
    "# --- SMART RESUME LOGIC ---\n",
    "print(f\"\\nChecking existing progress in s3://{BUCKET_NAME}/{OUTPUT_FOLDER}...\")\n",
    "\n",
    "# 1. Get list of files already in Output\n",
    "existing_md_files = set()\n",
    "try:\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=BUCKET_NAME, Prefix=OUTPUT_FOLDER):\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                # Store just the filename \"Report.md\"\n",
    "                existing_md_files.add(os.path.basename(obj['Key']))\n",
    "    print(f\"‚úì Found {len(existing_md_files)} already processed files.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not list output folder (assuming empty): {e}\")\n",
    "\n",
    "# 2. Get list of PDFs to process\n",
    "print(f\"Listing PDFs from s3://{BUCKET_NAME}/{INPUT_FOLDER}...\")\n",
    "pdf_files_to_process = []\n",
    "try:\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=BUCKET_NAME, Prefix=INPUT_FOLDER):\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                s3_key = obj['Key']\n",
    "                if s3_key.lower().endswith('.pdf'):\n",
    "                    # Check if this file is already done\n",
    "                    base_name = os.path.splitext(os.path.basename(s3_key))[0]\n",
    "                    expected_md = f\"{base_name}.md\"\n",
    "                    \n",
    "                    if expected_md in existing_md_files:\n",
    "                        print(f\"  ‚è≠Ô∏è  Skipping {base_name} (Already exists in output)\")\n",
    "                    else:\n",
    "                        pdf_files_to_process.append(s3_key)\n",
    "                        \n",
    "    print(f\"\\nüìã Pending files: {len(pdf_files_to_process)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error accessing S3 bucket: {e}\")\n",
    "    pdf_files_to_process = []\n",
    "\n",
    "\n",
    "# --- PROCESSING LOOP ---\n",
    "if pdf_files_to_process:\n",
    "    print(\"\\nStarting Batch Processing...\")\n",
    "    print('='*60)\n",
    "\n",
    "    start_time = time.time()\n",
    "    processed_count = 0\n",
    "    failed_files = []\n",
    "\n",
    "    # Download only the pending files\n",
    "    for idx, s3_key in enumerate(pdf_files_to_process, 1):\n",
    "        filename = os.path.basename(s3_key)\n",
    "        local_path = os.path.join('pdfs', filename)\n",
    "        \n",
    "        pdf_start = time.time()\n",
    "        base_name = os.path.splitext(filename)[0]\n",
    "        \n",
    "        print(f\"\\n[{idx}/{len(pdf_files_to_process)}] Processing: {filename}\")\n",
    "\n",
    "        # Download\n",
    "        if not os.path.exists(local_path):\n",
    "            print(f\"  Downloading...\")\n",
    "            s3_client.download_file(BUCKET_NAME, s3_key, local_path)\n",
    "\n",
    "        try:\n",
    "            # Create unique temp directory\n",
    "            temp_dir = f\"temp_conversion_{idx}\"\n",
    "            if os.path.exists(temp_dir): shutil.rmtree(temp_dir)\n",
    "            os.makedirs(temp_dir)\n",
    "\n",
    "            # Copy PDF to temp\n",
    "            temp_pdf_path = os.path.join(temp_dir, filename)\n",
    "            shutil.copy2(local_path, temp_pdf_path)\n",
    "            \n",
    "            print(f\"  Converting...\")\n",
    "            \n",
    "            clean_env = os.environ.copy()\n",
    "            clean_env[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "            \n",
    "            # Run marker\n",
    "            result = subprocess.run(\n",
    "                ['marker_single', filename],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                cwd=temp_dir,\n",
    "                env=clean_env\n",
    "            )\n",
    "            \n",
    "            # --- OUTPUT DETECTION LOGIC ---\n",
    "            md_found = False\n",
    "            \n",
    "            # 1. Try to find the path in the logs\n",
    "            log_output = result.stdout + result.stderr\n",
    "            match = re.search(r\"Saved markdown to\\s+(.*)\", log_output)\n",
    "            \n",
    "            search_paths = [temp_dir]\n",
    "            if match:\n",
    "                search_paths.append(match.group(1).strip())\n",
    "            \n",
    "            # 2. Add fallback path\n",
    "            fallback_path = \"/usr/local/lib/python3.12/dist-packages/conversion_results\"\n",
    "            if os.path.exists(fallback_path):\n",
    "                search_paths.append(fallback_path)\n",
    "\n",
    "            # 3. Search for the file\n",
    "            for search_path in search_paths:\n",
    "                if not os.path.exists(search_path): continue\n",
    "                    \n",
    "                for root, dirs, files in os.walk(search_path):\n",
    "                    for file in files:\n",
    "                        if file.lower().endswith('.md') and base_name.lower() in file.lower():\n",
    "                            source_md = os.path.join(root, file)\n",
    "                            dest_md = os.path.join('markdown_output', f\"{base_name}.md\")\n",
    "                            \n",
    "                            with open(source_md, 'r', encoding='utf-8') as f:\n",
    "                                content = f.read()\n",
    "                            \n",
    "                            if len(content) > 10:\n",
    "                                shutil.copy2(source_md, dest_md)\n",
    "                                \n",
    "                                # Upload to S3\n",
    "                                s3_key_out = f\"{OUTPUT_FOLDER}{base_name}.md\"\n",
    "                                s3_client.upload_file(dest_md, BUCKET_NAME, s3_key_out)\n",
    "                                print(f\"  ‚úì Success! Uploaded: {s3_key_out}\")\n",
    "                                \n",
    "                                md_found = True\n",
    "                                processed_count += 1\n",
    "                                break\n",
    "                    if md_found: break\n",
    "                if md_found: break\n",
    "\n",
    "            if not md_found:\n",
    "                print(f\"  ‚úó Failed. Error logs:\")\n",
    "                print(result.stderr[-500:] if result.stderr else \"No error output captured.\")\n",
    "                failed_files.append(filename)\n",
    "            \n",
    "            # Cleanup\n",
    "            if os.path.exists(temp_dir): shutil.rmtree(temp_dir)\n",
    "            # Optional: Remove local PDF to save space\n",
    "            if os.path.exists(local_path): os.remove(local_path)\n",
    "\n",
    "            # ETA\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time = elapsed / idx\n",
    "            remaining = (len(pdf_files_to_process) - idx) * avg_time\n",
    "            print(f\"  ‚è±Ô∏è  ETA: {remaining/60:.1f}min\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚úó Error: {e}\")\n",
    "            failed_files.append(filename)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Finished. Success: {processed_count} | Failed: {len(failed_files)}\")\n",
    "else:\n",
    "    print(\"üéâ All files are already processed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
